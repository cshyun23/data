#Deep NN에서의 문제점
  sigmoid함수를 사용할때, 2~3단은 학습이 잘되지만, Deep NN(layer이 9단,10단등)은 정확도가 떨어짐
  -> sigmoid함수를 사용하기때문에 backpropagation을 하면 미분값이 1보다 작고 0에 매우 가까운 수(0.03*0.02*....)로 수렴됨
   =최종값에 영향을 거의 못미치게됨
   
   ReLU(0보다 작을떄는 0, 0보다 클떄는 linear한 함수)->sigmoid대신에 사용(Deep NN에서 유용)
   ->tf.nn.relu
   (weight값을 모두 0으로 초기화하면 학습이 안됨)
   
   Leaky ReLU(0보다 작을때 0이아닌 작은기울기의linear)
   ELU(0보다작을떄 특정값을 나타냄)
   tanh(sigmoid와 비슷)
   
   
#W를 초기화하는 방법
   
   Deep belief Net(RBM을사용하여 초기화한 모델)
    Forward 와 backward의 x갑의 차가 최소가되는 w를 구함->이과정을 2개의layer사이에서 차례로 적용
    
    하지만은 복잡
    더간단한 초기화함수
노드의 입력값과 출력값에 비례하여 w를 초기화하는 방법
Xavier initialization
W = np.random.randn(fan_in,fan_out)/np.sqrt(fan_in)

He initialization
np.random.randn(fan_in,fan_out)/np.sqrt(fan_in/2)

#overfitting
  해결책 - more training
           regulationlereg = 0x001*tf.reduce_sum(tf.square(W))
           Dropout(연결을 끊어 overfitting을 막음)
    
-Dropout
dropout_rate = tf.placeholder("float")
_L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))
L1 = tf.nn.dropout(_L1, dropout_rate)

TRAIN:(학습할때는 일부만 트레이닝)
sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys,
dropout_rate: 0.7})

EVALUATION:(평가할땐 모든 데이터사용)
print "Accuracy:", accuracy.eval({X: mnist.test.images, Y:
mnist.test.labels, dropout_rate: 1})
